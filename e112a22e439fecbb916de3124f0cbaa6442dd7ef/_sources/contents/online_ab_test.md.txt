# Online A/B 테스트
### 샘플 크기 추정
Battleground 의 샘플 크기 추정은 빈도 통계학을 따라 정의되었습니다. 사전 가정(분산 동일 유무)의 차이에 따라 크게 두 가지 방법으로 구성이 되었으며 둘 모두 구현되어 있어서 사용자가 선택해서 활용할 수 있도록 하였습니다. 이 들의 수행은 아래의 수식을 따라 수행되었습니다. 첫 번째 수식은 스탠포트의 강의에서 발췌[1]한 샘플 크기 추정 수식입니다. 

![stanford](./images/stanford.png)

두 번째 수식은 Evan Miler 의 Post[2, 3]에서 발췌한 샘플 크기 추정 수식입니다. 

![evan](./images/evan.png)

이 방법들을 사용하기 위해서는 사전에 알아야하는 몇몇 값들이 있습니다. 
- p : 기존 알고리즘의 전환율입니다. 전환율의 정의는 클릭 혹은 가입 등 다양하게 정의될 수 있으며 예시에서는 클릭으로 진행될 예정입니다. 
- alpha : 1종 오류의 크기입니다. 1종 오류는 간단하게 말하면 맞는데 틀리다고 하는 경우를 의미합니다. 
- power : 2종 오류도 알아야합니다. 2종오류는 틀린데 맞다고 할 경우입니다. 
- expected : 새로운 알고리즘의 기대되는 전환율입니다. 예시에서는 Offline A/B 테스트의 결과를 입력하였습니다. 
- bucket_min_size : 기존 알고리즘의 버킷당 1일 최소 유입 유저 수 입니다. 이는 최소 몇일 간 테스트를 진행할 지 모니터링 기간을 계산하기 위함이며 계산된 샘플크기로부터 최소 유저수를 나누어 계산하며 1주일보다 작은 경우 1주일을 반환하게 됩니다. 
- mde_method : MDE 방법입니다. MDE 값은 Minimum detectable effect, 최소한의 검출 가능 효과를 의미하며 두 알고리즘의 전환율의 차이를 어느 정도로 볼 지 결정하는 값입니다. 현재는 두 알고리즘 각각의 전환율의 차이, 배수, Cohen's h가 구현되어있으며 기본값은 Cohen's h를 따라 계산하는 방법입니다. 그 외에도 사용자가 직접 추정한 두 알고리즘 간의 전환율의 차이를 직접 입력하여 계산할 수 있을 수 있도록 개발되었습니다. 
- case : 동일 분산의 가정 유무입니다. 각 사전에 정의된 값을 따라 계산되어 각 버킷 당 최소 샘플 수를 반환하게 됩니다. 기본 값은 동일 분산이 아니라는 가정하에 계산됩니다.

```
>>> #Online A/B test 사용 예시
>>> base_ctr = (bucket_a['reward'] == 1).sum() / bucket_a.shape[0]
>>> print("기존 CTR : ", base_ctr)
>>> print("Offline A/B test 결과 기대 CTR : ", cis_weight['CIS_weight'].mean())
기존 CTR :  0.01483019427554501
Offline A/B test 결과 기대 CTR :  0.06893937460876202

>>> sample_size_estimater = SampleSizeEstimater(alpha = 0.05
>>>                                             , power = 0.9
>>>                                             , p = base_ctr
>>>                                             , expected = cis_weight['CIS_weight'].mean())
>>> sample_size, require_periods = sample_size_estimater.cal_sample_size()
>>> print("비교하고자 하는 모델의 CTR 이 5% 임을 검증하려고 할때 필요한 실험(버킷) 당 최소 샘플 수 : ", sample_size)
비교하고자 하는 모델의 CTR 이 5% 임을 검증하려고 할때 필요한 실험(버킷) 당 최소 샘플 수 :  15954

>>> sample_size_estimater.get_distribution() #alpha 와 power 별 샘플 크기 plot
```
![dist](./images/dist.png)

### 전환 성능 비교
battleground 의 여러가지 성능 측정 방법들은 사용의 편리함을 위하여 동일한 형태를 기준으로 개발되었습니다. 대부분 Pandas DataFrame 을 받는 형식이며 파라미터는 컬럼명을 받는 등 sklearn 에서 수행하듯이 어느정도 정해진 형태의 데이터를 받아 수행할 수 있도록 수행 되었습니다. 성능을 더 빠르게 하기 위하여 list 나 array 를 받아 numpy로 연산할 수 있지만 사용자로 하여금 익숙해지면 손쉽게 쓸 수 있도록 하는 것에 초점을 맞추어 어느정도의 자유도를 제한하고 기준을 통일하여 진행하였습니다. 만약 추후 성능 이슈가 발생 시 성능을 올릴 수 있는 여러 trickly 한 방법들을 사용하여 추가로 개발을 지원할 예정이니 필요하시면 담당자에게 연락주시면 감사합니다. 또한 A/B 테스트 결과를 추정하는 것을 먼저 개발해두었습니다. A/B/C 나 A/B/C/D 등은 여건이 되는대로 점진적으로 고도화를 해가며 추가할 예정이니 참조하여 주시면 감사합니다.

#### 빈도 통계 기반
##### 카이제곱 검증
battleground 의 전환율 비교 성능은 크게 빈도통계 기반 방식과 베이지안 방식 2가지를 준비해두었습니다. 빈도 통계 방식은 카이제곱 검증과 비율 검증이 구현되어 있으며 카이제곱은 추천 유무 별 전환 유무(클릭/가입 등)를 검증하기 위한 목적으로 넣었습니다. 카이제곱 검증에 대한 자세한 내용은 [4]을 참조하시면 자세히 보실 수 있습니다. H0 는 추천 유무 별 전환 유무는 연관성이 없다 이며 H1은 추천 유무 별 전환 유무가 연관성이 있다 입니다. 즉, 추천 유무가 전환과의 연관성이 있는지를 검증하기 위한 목적으로 사용할 수 있습니다. 샘플 크기 추정을 통해 어느정도 수 이상의 샘플을 테스트 할 것이라고 가정하였기 때문에 피셔의 정확도 검증은 구현되어 있지 않습니다.

```
>>> #Chi sqare test 사용 예시
>>> chi_squ = ConversionEstimater(control = bucket_a[['ci', 'prod_id', 'reward']]
>>>                               , target = random_bucket[['ci', 'prod_id', 'reward']]
>>>                               , control_reward_col = 'reward'
>>>                               , target_reward_col = 'reward'
>>>                               , control_reward_val = 1
>>>                               , target_reward_val = 1)
>>> chi2, p_val, df, expected, effect_size = chi_squ.chi_square_val()
>>> if p_val < 0.05 :
>>>     print("추천 유무 별 전환이 연관성이 있다고 주장할 수 있다. 즉, 추천 유무가 전환에 영향을 준다고 주장할 수 있는 충분한 근거를 얻었다.")
>>> else:
>>>     print("추천 유무 별 전환이 연관성이 있다고 주장할 수 없다. 즉, 추천 유무가 전환에 영향을 준다고 주장할 수 있는 충분한 근거를 얻지 못하였다.")
추천 유무 별 전환이 연관성이 있다고 주장할 수 있다. 즉, 추천 유무가 전환에 영향을 준다고 주장할 수 있는 충분한 근거를 얻었다. #추천 알고리즘이 전환에 영향을 준다고 할 수 있다.
```

반환되는 결과는 카이제곱 통계량과 p value, 자유도, 기대값, 효과크기입니다. 반환된 결과를 기반으로 영향을 준다고 볼 수 있는지를 검증할 수 있으며 효과크기를 통해 얼만큼 영향을 준다고 볼 수 있는지를 검증할 수 있습니다. 카이제곱 실행 시 자유도가 5이하인 경우에 대해서 효과 크기가 어느 정도 차이가 나는지를 일반적으로 해석하는 DataFrame을 출력해줍니다. 예를 들어, 대조군 x 처지군의 클릭/미클릭의 경우 자유도가 1이므로 자유도가 1일 경우 일반적으로 효과크기를 해석하는 기준을 아래와 같이 출력해줍니다. 

![degree](./images/degree.png)

하지만 이 크기가 절대적인 것은 아니며 상황과 데이터에 따라 p-value와 같이 연구자가 해석하기 나름이라는 점을 유의해야합니다.

##### 비율 검증
battleground 에 구현된 빈도 통계 기반 비율 검증은 z 값을 통해 계산하며 추천 알고리즘 별 전환 유무(클릭/가입 등)를 검증하기 위한 목적으로 개발되었습니다. 비율 검증은 [6]를 참조하시면 자세한 내용을 보실 수 있습니다. H0는 두 집단간 비율에 차이가 없다이며 H1은 차이가 있다 입니다. 즉, 추천 알고리즘 별 전환 비율에 차이가 있는지를 검증하기 위한 목적으로 사용할 수 있으며 샘플 크기 추정을 통해 어느정도 수 이상의 샘플을 테스트 할 것이라고 가정하였기 때문에 분산 정규성 검증 등은 구현되어 있지 않습니다.

```     
>>> #z test 사용 예시
>>> z_test = ConversionEstimater(control = bucket_a[['ci', 'prod_id', 'reward']]
>>>                              , target = random_bucket[['ci', 'prod_id', 'reward']]
>>>                              , control_reward_col = 'reward'
>>>                              , target_reward_col = 'reward'
>>>                              , control_reward_val = 1
>>>                              , target_reward_val = 1)
>>> control_prop, treatment_prop, z_stat, p_value, control_confidence_interval_control, treatment_confidence_interval_control, effect_size = z_test.prop_test()            
>>> if p_val < 0.05 :
>>>     print("추천 유무 별 전환 비율이 같다고 주장할 수 있다. 즉, 추천 유무가 전환에 영향을 준다고 주장할 수 있는 충분한 근거를 얻었다.")
>>> else:
>>>     print("추천 유무 별 전환 비율이 같다고 주장할 수 없다. 즉, 추천 유무가 전환에 영향을 준다고 주장할 수 있는 충분한 근거를 얻지 못하였다.")
추천 유무 별 전환 비율이 같다고 주장할 수 있다. 즉, 추천 유무가 전환에 영향을 준다고 주장할 수 있는 충분한 근거를 얻었다. #추천 알고리즘이 전환에 영향을 준다고 할 수 있다.
```

반환되는 결과는 대조군 전환비율, 타겟군 전환비율, z 통계량과 p value, 대조군 신뢰구간, 타겟군 신뢰구간, 효과크기입니다. 반환된 결과를 기반으로 영향을 준다고 볼 수 있는지를 검증할 수 있으며 효과크기를 통해 얼만큼 영향을 준다고 볼 수 있는지를 검증할 수 있습니다. 카이제곱과 마찬가지로 이 크기가 절대적인 것은 아니며 상황과 데이터에 따라 p-value와 같이 연구자가 해석하기 나름이라는 점을 유의해야합니다.

#### 베이지안 기반
##### 베이지안 A/B 테스트
베이지안 검증은 A/B, A/B/C, A/B/C/D 테스트가 가능하도록 준비되어 있습니다. Battleground 에 구현되어있는 베이지안 검증은 보상이 Binary 인 경우에 한해 개발되었으니 사용에 유의를 해야합니다. 베이지안 A/B 계산은 아래의 수식을 따라 진행 됩니다. 아래의 수식은 유사한 결과를 산출하게 되며 자세한 증명은 [5]을 참조하시면 알 수 있습니다.

![ab](./images/ab.png)
![ab2](./images/ab2.png)

- 𝛼𝐴 : A 알고리즘의 전환(클릭 혹은 가입 등) 성공 수
- 𝛽𝐴 : A 알고리즘의 전환(클릭 혹은 가입 등) 실패 수
- 𝛼𝐵 : B 알고리즘의 전환(클릭 혹은 가입 등) 성공 수
- 𝛽𝐵 : B 알고리즘의 전환(클릭 혹은 가입 등) 실패 수
- 𝐵 : 베타 함수

```     
>>> # 베이지안 A/B 테스트 사용 예시
>>> online_abtesting = ConversionEstimater(control = bucket_a
>>>                                        , target = bucket_b_example
>>>                                        , control_reward_col = 'reward'
>>>                                        , target_reward_col = 'reward')
>>> control_click_rate, target_click_rate, prob_b_a = online_abtesting.cal_bayes_prob()
>>> print("Control Grp Click rate : ", control_click_rate)
>>> print("Target Grp Click rate : ", target_click_rate)
>>> print("probability Pt > Pc : ", prob_b_a)
>>> print("지수형 -> 숫자형 : ", '{:.8f}'.format(prob_b_a))
Control Grp Click rate :  0.01483019427554501
Target Grp Click rate :  0.0015598692680994354
probability Pt > Pc :  3.7421643192430597e-113
지수형 -> 숫자형 :  0.00000000
```  
   
반환되는 결과는 대조군 전환율, 타겟군 전환율, 타겟군이 대조군 대비 더 많이 전환될 확률입니다. 반환된 결과를 기반으로 영향을 준다고 볼 수 있는지를 검증할 수 있습니다. "집단 별 효과가 없다면 무작위 표본 추출 오류로 인해 연구의 n%에서 관찰된 수준 이상의 차이가 도출된다" 라고 해석해야하는 빈도 통계 기반 방법들과 다르게 베이지안 방법은 보다 직관적으로 "타겟군이 대조군 대비 더 많이 전환될 확률이 n이다"라고 해석할 수 있습니다. 빈도 통계 기반의 방법들은 모집단에 대해 귀무 가설이 참이며 표본의 차이가 오로지 무작위 확률에 의해 발생했다는 가정에 기반하여 계산되기 때문에 귀무 가설의 참, 거짓 확률을 나타낼 수 없는데 반해 베이지안 방법은 직관적으로 해석할 수 있습니다.
                                                
##### 베이지안 A/B/C 테스트
베이지안 A/B/C 검증은 아래의 수식을 따라 진행 됩니다. 자세한 증명은 [8]을 참조하시면 알 수 있습니다.

![abc](./images/abc.png)

𝛼𝑋 : A, B, C 알고리즘 각각의 전환(클릭 혹은 가입 등) 성공 수, 𝑋∈{𝐴,𝐵,𝐶}.
𝛽𝑋 : A, B, C 알고리즘 각각의 전환(클릭 혹은 가입 등) 실패 수, 𝑋∈{𝐴,𝐵,𝐶}.
Pr(𝑝𝑋>𝑝𝐶) : 베이지안 A/C, B/C 검증 결과, 𝑋∈{𝐴,𝐵,𝐶}.

```     
>>> # 베이지안 A/B/C 테스트 사용 예시
>>> online_abtesting = ConversionEstimater(control = bucket_a
>>>                                        , target = bucket_b_example
>>>                                        , control_reward_col = 'reward'
>>>                                        , target_reward_col = 'reward'
>>>                                        , control2 = bucket_c)
>>> control_click_rate, control2_click_rate, target_click_rate, prob_b_a = online_abtesting.cal_multi_bayes_prob()
>>> print("Control 1 Grp Click rate : ", control_click_rate)
>>> print("Control 2 Grp Click rate : ", control2_click_rate)
>>> print("Target Grp Click rate : ", target_click_rate)
>>> print("probability Pt > Pc : ", prob_t_c)
>>> print("지수형 -> 숫자형 : ", '{:.8f}'.format(prob_t_c))
Control 1 Grp Click rate :  0.01483019427554501
Control 2 Grp Click rate :  0.02323984298374124
Target Grp Click rate :  0.0015598692680994354
probability Pt > Pc :  3.7421643192430597e-113
지수형 -> 숫자형 :  0.00000000
```  
   
반환되는 결과는 대조군 1 전환율, 대조군 2 전환율, 타겟군 전환율, 타겟군이 대조군 대비 더 많이 전환될 확률입니다. 베이지안 A/B 테스트와 동일하게 해석할 수 있습니다.
 
##### 베이지안 A/B/C/D 테스트
베이지안 A/B/C/D 검증은 아래의 수식을 따라 진행 됩니다. 자세한 증명은 [8]을 참조하시면 알 수 있습니다.

![abcd](./images/abcd.png)

```     
>>> # 베이지안 A/B/C/D 테스트 사용 예시
>>> online_abtesting = ConversionEstimater(control = bucket_a
>>>                                        , target = bucket_b_example
>>>                                        , control_reward_col = 'reward'
>>>                                        , target_reward_col = 'reward'
>>>                                        , control2 = bucket_c
>>>                                        , control3 = bucket_d)
>>> control_click_rate, control2_click_rate, control3_click_rate, target_click_rate, prob_b_a = online_abtesting.cal_multi_bayes_prob()
>>> print("Control 1 Grp Click rate : ", control_click_rate)
>>> print("Control 2 Grp Click rate : ", control2_click_rate)
>>> print("Control 3 Grp Click rate : ", control3_click_rate)
>>> print("Target Grp Click rate : ", target_click_rate)
>>> print("probability Pt > Pc : ", prob_t_c)
>>> print("지수형 -> 숫자형 : ", '{:.8f}'.format(prob_t_c))
Control 1 Grp Click rate :  0.01483019427554501
Control 2 Grp Click rate :  0.02323984298374124
Control 3 Grp Click rate :  0.01231827391872837
Target Grp Click rate :  0.0015598692680994354
probability Pt > Pc :  3.7421643192430597e-113
지수형 -> 숫자형 :  0.00000000
```  
   
반환되는 결과는 대조군 1 전환율, 대조군 2 전환율, 대조군 3 전환율, 타겟군 전환율, 타겟군이 대조군 대비 더 많이 전환될 확률입니다. 베이지안 A/B/C 테스트와 동일하게 해석할 수 있습니다.
  
### 랭킹 기반 추천 성능 

랭킹 기반 추천 성능은 MAP(Mean Average Precision), MAR(Mean Average Recall), nDCG(Normailzed Discounted Cumulative Gain)가 구현되어 있습니다. 사용에 유의할 점은 저희는 로그를 DB로부터 pandas 형태로 불러오는 것에 익숙하여 이를 그대로 넣을 수 있도록 개발이 되었는데 MAP나 MAR, nDCG 등을 계산할 때 추천의 순서 별로 정렬하여 넣어야 한다는 점입니다. 별도로 정렬 로직을 추가하려고 하다가 추천을 담당하는 담당자가 추천의 순서를 잘 알테니 과제마다 다른 경우를 고려하지 않고 정렬 로직을 심는 것 보다 추천의 내용과 데이터를 잘 아는 담당자가 정렬해서 데이터를 넣어 연산할 수 있도록 개발되었습니다. 유저 별로 가장 첫 번째로 추천되는 아이템을 처음으로 등장하도록 정렬한 뒤에 함수에 넣으면 그 순서를 기반으로 인덱싱을 수행하므로 정렬을 하지 않으면 제대로 된 결과를 출력하지 않을 수 있습니다.

#### MAP
추천 시스템에서는 사용자가 관심을 더 많이 가질만한 아이템을 상위에 추천해주는 것이 매우 중요합니다. 이를 위해 성능 평가에 순서 개념을 도입한 것이 Mean Average Precision @ K 입니다. 계산은 아래의 그림을 보시면 이해하실 수 있습니다.

![map](./images/map.png)

```     
>>> # MAP(Mean Average Precision) 사용 예시
>>> online_abtesting = RankingEstimater(rec = bucket_a
>>>                                     , item_col = 'prod_id'
>>>                                     , user_col = 'ci'
>>>                                     , reward = 'click_yn'
>>>                                     , k = 3)        
>>> map_cal = online_abtesting.mean_average_precision()
>>> print("Mean Average Precision : ", map_cal)
Mean Average Precision : 0.5
```  

#### MAR
MAP 가 Precision에 기반해 만들어졌다면 MAR은 Recall에 기반하여 만들어졌습니다. MAP계산에서 Precision 대신에 Recall을 대입하여 계산하게 됩니다.

```     
>>> # MAR(Mean Average Recall) 사용 예시
>>> online_abtesting = RankingEstimater(rec = bucket_a
>>>                                     , item_col = 'prod_id'
>>>                                     , user_col = 'ci'
>>>                                     , reward = 'click_yn'
>>>                                     , k = 3
>>>                                     , reco_col = 'impression_yn')        
>>> mar_cal = online_abtesting.mean_average_recall()
>>> print("Mean Average Recall : ", mar_cal)
Mean Average Recall : 0.5
```  

#### nDCG
nDCG는 원래 검색 분야에서 등장한 지표이나 추천 시스템에도 많이 사용되고 있습니다. Top K개 아이템을 추천하는 경우, 추천 순서에 가중치를 두어 평가합니다. NDCG @ K값은 1에 가까울수록 좋은 성능을 보이는 것이며. 순서별로 가중치 값(관련도, relevance)을 다르게 적용하여 계산합니다. 계산은 아래의 그림을 보시면 이해하실 수 있습니다.

![ndcg](./images/ndcg.png)

```     
>>> # MAP(Mean Average Precision) 사용 예시
>>> online_abtesting = RankingEstimater(rec = bucket_a
>>>                                     , item_col = 'prod_id'
>>>                                     , user_col = 'ci'
>>>                                     , reward = 'click_yn'
>>>                                     , k = 3)        
>>> ndcg = online_abtesting.ndcg()
>>> print("Normailzed Discounted Cumulative Gain : ", ndcg)
Normailzed Discounted Cumulative Gain : 0.5
```  

### 연속 값 차이 추정
battleground에는 Binary Conversion 성능 추정에 맞추어 개발이 되어 있습니다. 이유는 현재 T3K의 KPI는 비즈니스와 연관되어 CTR 향상을 해야하는 상황이고, CTR은 세부적으로 보면 클릭 한다/안한다의 action이 2개이기 때문에 Binary Conversion 성능 추정이 필요한 상황이고 현재 시점에서 아무런 것도 개발이 안되어있기 때문에 우선순위를 맞추어 Binary Conversion 성능 추정에 초점을 맞추어 개발하였습니다.

그러나 A/B 테스트는 Binary Conversion 외에도 다양한 방법으로 진행이 될 수 있습니다. 예를 들어, 페이지에 머문 시간, 페이지에 어느 위치까지 보았는지, 총 매출 등 다양한 환경에서 진행할 수 있기 때문에 연속적인 값(Continuous)도 성능을 추정할 수 있도록 하기 위하여 개발되었습니다. 

#### 독립 표본 T 검증
A/B 테스트를 위해 독립적인 두 모집단에서 추출된 샘플 간 비교를 위해 독립 표본 T검증을 준비해두었습니다. 자세한 내용은 [7]을 참조하시면 보실 수 있습니다. 사용 예시는 아래와 같습니다.

```  
>>> control = stats.norm.rvs(loc=5, scale=10, size=500, random_state=1)
>>> target = stats.norm.rvs(loc=5, scale=10, size=500, random_state=1)
>>> ttest = ContinuousCompare(control = control
>>>                           , target = target)        
>>> t_stat, p_val, effect_size = ttest.ttest()
>>> if p_val < 0.05 :
>>>     print("집단 별 평균이 같지 않다고 주장할 충분한 근거를 얻지 못하였다.")
>>> else:
>>>     print("집단 별 평균이 같지 않다고 주장할 충분한 근거를 얻었다.")
집단 별 평균이 같지 않다고 주장할 충분한 근거를 얻었다.
```  

반환되는 결과는 크게 세가지로 t통계량, p value, 효과크기를 반환합니다. 이를 통해 두 집단 간 표본 평균으로 추정한 모집단의 평균이 차이가 있는지, 있다면 어느정도 차이로 차이가 있는지를 볼 수 있습니다.

#### 베이지안 T 검증
베이지안 T 검증은 [8]에 기반하여 개발되었습니다. 자세한 내용은 [8]을 참조하시면 보실 수 있습니다. 아래는 사용 예시입니다.

```  
>>> control = stats.norm.rvs(loc=5, scale=10, size=500, random_state=1)
>>> target = stats.norm.rvs(loc=5, scale=10, size=500, random_state=1)
>>> diff_val = 0.01
>>> ttest = ContinuousCompare(control = control
>>>                           , target = target
>>>                           , diff = diff_val)        
>>> prob = ttest.bayes_ttest()
>>> print(f"target 그룹의 평균이 control 그룹의 평균보다 {diff_val} 만큼 클 확률 : ", prob)
target 그룹의 평균이 control 그룹의 평균보다 0.01 만큼 클 확률 : 0.6
```  

-------------------
[1] https://statistics.stanford.edu/technical-reports/determining-appropriate-sample-size-confidence-limits-proportion

[2] https://www.evanmiller.org/ab-testing/sample-size-fixed.js

[3] https://www.evanmiller.org/ab-testing/sample-size.html

[4] https://en.wikipedia.org/wiki/Chi-squared_test

[5] https://www.evanmiller.org/bayesian-ab-testing.html#count_ab

[6] https://vitalflux.com/two-sample-z-test-for-proportions-formula-examples/

[7] https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html

[8] https://best.readthedocs.io/en/latest/index.html
